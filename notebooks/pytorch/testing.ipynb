{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23dac719-d50e-4415-b0e1-78b5c69871ec",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Testing notebook for animal classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a811900-0ab6-4cc7-9bc7-f30fb0dffc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.1.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Pytorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f003a-bbd7-40f2-99d7-ff8b8f8a99c5",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676429a3-d717-40b7-9d1d-bd1497d5c639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# GPU or CPU?\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4e2d3-955b-400b-a604-b1a16053e3b9",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "1. Make sure the animals dataset is downloaded into the \"datasets/animals\" directory\n",
    "   (https://www.kaggle.com/datasets/npurav/animal-classification-dataset)\n",
    "2. Add augmented images to the dataset.\n",
    "3. Load the dataset then split into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d4f5a79-fbfb-4ee8-abed-fea09d0f3250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes: 117\n"
     ]
    }
   ],
   "source": [
    "# Get root directory\n",
    "BASE_DIR = Path.cwd().resolve().parent.parent\n",
    "\n",
    "# Path to your dataset\n",
    "dataset_path = BASE_DIR / 'datasets/animals/dataset'\n",
    "\n",
    "# Define transformations (you can customize these based on your needs)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a common size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create ImageFolder dataset\n",
    "dataset = ImageFolder(dataset_path, transform=transform)\n",
    "class_names = dataset.classes\n",
    "print(f'Total number of classes: {len(class_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b222d8b-ce1d-4aa4-b1f4-0b63105d7bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 19225\n"
     ]
    }
   ],
   "source": [
    "# Image augmentation\n",
    "# Define image transformations for augmentation\n",
    "augmentation_transform = transforms.Compose([\n",
    "    #transforms.RandomResizedCrop(224),        # Random crop and resize\n",
    "    transforms.RandomHorizontalFlip(),        # Random horizontal flip\n",
    "    transforms.RandomVerticalFlip(),          # Random vertical flip\n",
    "    # transforms.RandomRotation(degrees=15),    # Random rotation (up to 15 degrees)\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n",
    "    transforms.RandomGrayscale(p=0.1),        # Randomly convert to grayscale\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.2),  # Random perspective\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Create augmented dataset\n",
    "for i in range(1):\n",
    "    # Apply augmentation to the entire dataset\n",
    "    augmented_dataset = ImageFolder(dataset_path, transform=augmentation_transform)\n",
    "    # Combine the original and augmented datasets\n",
    "    dataset = augmented_dataset\n",
    "    \n",
    "print(f'Total number of images: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53818714-cd3b-4ec0-8b92-3ad9d09d6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 64\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f50563ac-2414-423b-a436-f804264e8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(3, 3)\n",
    "        self.conv2 = nn.Conv2d(16, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.fc1 = nn.Linear(6272, 3000)\n",
    "        self.fc2 = nn.Linear(3000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, len(class_names))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3186433c-8eb6-4d2b-ad9b-e6e5e9e06736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 22021469\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f'Trainable params: {sum(p.numel() for p in net.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52486333-bd5d-4631-8f5d-38299f12d705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "large = 'models/ac_large.pth'\n",
    "medium = 'models/ac_med.pth'\n",
    "medium = 'models/ac_small.pth'\n",
    "PATH = BASE_DIR / large\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66ac938b-73d9-43ff-bd7f-5592f1f30aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset: 91.46%\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the test dataset: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
